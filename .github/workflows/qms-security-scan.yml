name: QMS Security Scanning Pipeline
run-name: ðŸ›¡ï¸ Security Scan - ${{ github.event_name == 'pull_request' && format('PR #{0}', github.event.number) || github.ref_name }} by @${{ github.actor }}

on:
  push:
    branches: [ main, develop, release/* ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    - cron: '0 2 * * 1' # Weekly security scan on Mondays at 2 AM UTC
  workflow_dispatch:
    inputs:
      scan_type:
        description: 'Type of security scan to perform'
        required: false
        default: 'full'
        type: choice
        options:
          - full
          - sast-only
          - dast-only
          - sca-only
          - secrets-only
          - compliance-only
      severity_threshold:
        description: 'Minimum severity level to report'
        required: false
        default: 'medium'
        type: choice
        options:
          - low
          - medium
          - high
          - critical
      fail_on_severity:
        description: 'Fail build on this severity level and above'
        required: false
        default: 'high'
        type: choice
        options:
          - low
          - medium
          - high
          - critical

env:
  QMS_SECURITY_SCAN_VERSION: "v2.3.0"
  QMS_SCAN_TYPE: ${{ github.event.inputs.scan_type || 'full' }}
  QMS_SEVERITY_THRESHOLD: ${{ github.event.inputs.severity_threshold || 'medium' }}
  QMS_FAIL_ON_SEVERITY: ${{ github.event.inputs.fail_on_severity || 'high' }}
  QMS_DASHBOARD_ENDPOINT: ${{ vars.QMS_DASHBOARD_ENDPOINT || 'https://qms-dashboard.internal' }}
  QMS_COMPLIANCE_ENDPOINT: ${{ vars.QMS_COMPLIANCE_ENDPOINT || 'https://compliance.internal/api' }}
  QMS_EXECUTIVE_WEBHOOK: ${{ vars.QMS_EXECUTIVE_WEBHOOK }}
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

permissions:
  contents: read
  security-events: write
  actions: read
  issues: write
  pull-requests: write

concurrency:
  group: qms-security-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # Phase 1: Security Scan Preparation
  security-preparation:
    name: ðŸ”§ Security Scan Preparation
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      scan-matrix: ${{ steps.detect-languages.outputs.matrix }}
      scan-id: ${{ steps.generate-scan-id.outputs.scan-id }}
      baseline-ref: ${{ steps.baseline.outputs.ref }}
      compliance-required: ${{ steps.compliance-check.outputs.required }}
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Generate Security Scan ID
        id: generate-scan-id
        run: |
          SCAN_ID="SEC-$(date +%Y%m%d-%H%M%S)-${{ github.run_number }}"
          echo "scan-id=$SCAN_ID" >> $GITHUB_OUTPUT
          echo "ðŸ†” Security Scan ID: $SCAN_ID"

      - name: Detect Project Languages
        id: detect-languages
        run: |
          # Create language detection script
          cat > detect_languages.py << 'EOF'
          import os
          import json
          import subprocess

          def detect_languages():
              languages = set()
              
              # Language file extensions mapping
              extensions = {
                  '.js': 'javascript',
                  '.jsx': 'javascript',
                  '.ts': 'typescript',
                  '.tsx': 'typescript',
                  '.py': 'python',
                  '.java': 'java',
                  '.kt': 'kotlin',
                  '.cs': 'csharp',
                  '.cpp': 'cpp',
                  '.c': 'cpp',
                  '.go': 'go',
                  '.rs': 'rust',
                  '.rb': 'ruby',
                  '.php': 'php',
                  '.swift': 'swift',
                  '.scala': 'scala',
                  '.sh': 'shell',
                  '.tf': 'terraform',
                  '.yaml': 'yaml',
                  '.yml': 'yaml',
                  '.json': 'json',
                  '.xml': 'xml'
              }
              
              # Scan directory for files
              for root, dirs, files in os.walk('.'):
                  # Skip common ignore directories
                  dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['node_modules', '__pycache__', 'target', 'build', 'dist']]
                  
                  for file in files:
                      _, ext = os.path.splitext(file.lower())
                      if ext in extensions:
                          languages.add(extensions[ext])
              
              # Add Docker if Dockerfile exists
              if os.path.exists('Dockerfile') or os.path.exists('docker-compose.yml'):
                  languages.add('docker')
              
              # Add infrastructure if terraform files exist
              if any(f.endswith('.tf') for f in os.listdir('.') if os.path.isfile(f)):
                  languages.add('terraform')
              
              return list(languages)

          if __name__ == "__main__":
              langs = detect_languages()
              matrix = {
                  "language": langs if langs else ["generic"],
                  "include": []
              }
              
              # Add specific configurations for each language
              for lang in langs:
                  config = {"language": lang}
                  if lang == "javascript":
                      config.update({
                          "sast_tools": ["eslint-security", "semgrep", "nodejs-scan"],
                          "package_manager": "npm"
                      })
                  elif lang == "typescript":
                      config.update({
                          "sast_tools": ["tslint-security", "semgrep", "typescript-eslint"],
                          "package_manager": "npm"
                      })
                  elif lang == "python":
                      config.update({
                          "sast_tools": ["bandit", "semgrep", "pysa", "safety"],
                          "package_manager": "pip"
                      })
                  elif lang == "java":
                      config.update({
                          "sast_tools": ["spotbugs", "semgrep", "pmd"],
                          "package_manager": "maven"
                      })
                  elif lang == "go":
                      config.update({
                          "sast_tools": ["gosec", "semgrep", "staticcheck"],
                          "package_manager": "go"
                      })
                  elif lang == "docker":
                      config.update({
                          "sast_tools": ["hadolint", "dockle", "trivy"],
                          "package_manager": "docker"
                      })
                  else:
                      config.update({
                          "sast_tools": ["semgrep"],
                          "package_manager": "generic"
                      })
                  
                  matrix["include"].append(config)
              
              print(json.dumps(matrix, indent=2))
          EOF
          
          python detect_languages.py > language_matrix.json
          MATRIX=$(cat language_matrix.json)
          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
          echo "ðŸ” Detected Languages Matrix:"
          cat language_matrix.json

      - name: Determine Baseline Reference
        id: baseline
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "ref=${{ github.event.pull_request.base.sha }}" >> $GITHUB_OUTPUT
            echo "ðŸ”„ Using PR base as baseline: ${{ github.event.pull_request.base.sha }}"
          else
            echo "ref=${{ github.sha }}" >> $GITHUB_OUTPUT
            echo "ðŸ”„ Using current commit as baseline: ${{ github.sha }}"
          fi

      - name: Check Compliance Requirements
        id: compliance-check
        run: |
          # Check if this is a compliance-required repository
          COMPLIANCE_REQUIRED="false"
          if [ -f ".qms/compliance.yml" ] || [ -f "COMPLIANCE.md" ] || [[ "${{ github.repository }}" =~ ^.*/(prod-|production-|critical-).* ]]; then
            COMPLIANCE_REQUIRED="true"
          fi
          echo "required=$COMPLIANCE_REQUIRED" >> $GITHUB_OUTPUT
          echo "ðŸ›ï¸ Compliance Required: $COMPLIANCE_REQUIRED"

      - name: Initialize Security Dashboard
        run: |
          curl -X POST "${{ env.QMS_DASHBOARD_ENDPOINT }}/api/security/scans" \
            -H "Authorization: Bearer ${{ secrets.QMS_API_TOKEN }}" \
            -H "Content-Type: application/json" \
            -d '{
              "scan_id": "${{ steps.generate-scan-id.outputs.scan-id }}",
              "repository": "${{ github.repository }}",
              "branch": "${{ github.ref_name }}",
              "commit": "${{ github.sha }}",
              "trigger": "${{ github.event_name }}",
              "scan_type": "${{ env.QMS_SCAN_TYPE }}",
              "status": "started",
              "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"
            }' || true

  # Phase 2: Static Application Security Testing (SAST)
  sast-analysis:
    name: ðŸ” SAST Analysis - ${{ matrix.language }}
    runs-on: ubuntu-latest
    needs: security-preparation
    if: ${{ github.event.inputs.scan_type == 'sast-only' || github.event.inputs.scan_type == null || github.event.inputs.scan_type == '' || github.event.inputs.scan_type == 'full' }}
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.security-preparation.outputs.scan-matrix) }}
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Language Environment - ${{ matrix.language }}
        run: |
          case "${{ matrix.language }}" in
            javascript|typescript)
              node --version
              npm --version
              ;;
            python)
              python --version
              pip --version
              ;;
            java)
              java -version
              mvn -version || echo "Maven not available"
              ;;
            go)
              go version
              ;;
            *)
              echo "Generic language setup"
              ;;
          esac

      - name: Install SAST Tools - ${{ matrix.language }}
        run: |
          # Install Semgrep (universal)
          pip install semgrep

          # Install language-specific tools
          case "${{ matrix.language }}" in
            javascript|typescript)
              npm install -g eslint @typescript-eslint/parser @typescript-eslint/eslint-plugin eslint-plugin-security
              ;;
            python)
              pip install bandit safety
              ;;
            java)
              # Download SpotBugs
              wget https://github.com/spotbugs/spotbugs/releases/download/4.8.0/spotbugs-4.8.0.tgz
              tar -xzf spotbugs-4.8.0.tgz
              ;;
            go)
              go install github.com/securecodewarrior/gosec/v2/cmd/gosec@latest
              go install honnef.co/go/tools/cmd/staticcheck@latest
              ;;
            docker)
              # Install Hadolint
              wget -O hadolint https://github.com/hadolint/hadolint/releases/latest/download/hadolint-Linux-x86_64
              chmod +x hadolint
              sudo mv hadolint /usr/local/bin/
              
              # Install Dockle
              wget -O dockle.deb https://github.com/goodwithtech/dockle/releases/latest/download/dockle_amd64.deb
              sudo dpkg -i dockle.deb || sudo apt-get install -f
              ;;
          esac

      - name: Run SAST Analysis - ${{ matrix.language }}
        run: |
          mkdir -p sast-reports
          
          echo "ðŸ” Running SAST analysis for ${{ matrix.language }}"
          
          # Run Semgrep (universal)
          semgrep --config=auto --json --output=sast-reports/semgrep-${{ matrix.language }}.json . || true
          
          # Run language-specific tools
          case "${{ matrix.language }}" in
            javascript|typescript)
              # ESLint Security Analysis
              npx eslint . --ext .js,.jsx,.ts,.tsx --format json --output-file sast-reports/eslint-${{ matrix.language }}.json || true
              ;;
            python)
              # Bandit Security Analysis
              bandit -r . -f json -o sast-reports/bandit-python.json || true
              
              # Safety Check for Dependencies
              safety check --json --output sast-reports/safety-python.json || true
              ;;
            java)
              # SpotBugs Analysis
              if [ -f pom.xml ]; then
                mvn compile || true
                ./spotbugs-4.8.0/bin/spotbugs -textui -output sast-reports/spotbugs-java.xml target/classes || true
              fi
              ;;
            go)
              # Gosec Analysis
              gosec -fmt json -out sast-reports/gosec-go.json ./... || true
              
              # Staticcheck Analysis
              staticcheck -f json ./... > sast-reports/staticcheck-go.json || true
              ;;
            docker)
              # Hadolint Analysis
              find . -name "Dockerfile*" -exec hadolint {} \; > sast-reports/hadolint-docker.txt || true
              
              # Docker Compose Analysis
              if [ -f docker-compose.yml ]; then
                echo "Analyzing docker-compose.yml" >> sast-reports/docker-compose-analysis.txt
              fi
              ;;
          esac
          
          echo "ðŸ“Š SAST analysis complete for ${{ matrix.language }}"
          ls -la sast-reports/

      - name: Process SAST Results - ${{ matrix.language }}
        run: |
          # Create unified SAST report
          cat > process_sast.py << 'EOF'
          import json
          import os
          import glob
          from datetime import datetime

          def process_sast_reports():
              unified_report = {
                  "scan_id": "${{ needs.security-preparation.outputs.scan-id }}",
                  "language": "${{ matrix.language }}",
                  "timestamp": datetime.utcnow().isoformat() + "Z",
                  "findings": [],
                  "summary": {
                      "total_issues": 0,
                      "critical": 0,
                      "high": 0,
                      "medium": 0,
                      "low": 0,
                      "info": 0
                  },
                  "tools_used": []
              }
              
              # Process all JSON reports
              for report_file in glob.glob("sast-reports/*.json"):
                  try:
                      with open(report_file, 'r') as f:
                          data = json.load(f)
                      
                      tool_name = os.path.basename(report_file).split('-')[0]
                      unified_report["tools_used"].append(tool_name)
                      
                      # Process different tool formats
                      if tool_name == "semgrep":
                          for result in data.get("results", []):
                              finding = {
                                  "tool": "semgrep",
                                  "rule_id": result.get("check_id", "unknown"),
                                  "severity": map_severity(result.get("extra", {}).get("severity", "info")),
                                  "message": result.get("extra", {}).get("message", ""),
                                  "file": result.get("path", ""),
                                  "line": result.get("start", {}).get("line", 0),
                                  "category": "SAST"
                              }
                              unified_report["findings"].append(finding)
                              unified_report["summary"]["total_issues"] += 1
                              unified_report["summary"][finding["severity"]] += 1
                      
                      elif tool_name == "bandit":
                          for result in data.get("results", []):
                              severity = map_bandit_severity(result.get("issue_severity", "LOW"))
                              finding = {
                                  "tool": "bandit",
                                  "rule_id": result.get("test_id", "unknown"),
                                  "severity": severity,
                                  "message": result.get("issue_text", ""),
                                  "file": result.get("filename", ""),
                                  "line": result.get("line_number", 0),
                                  "category": "SAST"
                              }
                              unified_report["findings"].append(finding)
                              unified_report["summary"]["total_issues"] += 1
                              unified_report["summary"][severity] += 1
                      
                      # Add more tool processors as needed
                      
                  except Exception as e:
                      print(f"Error processing {report_file}: {e}")
              
              # Save unified report
              with open(f"sast-reports/unified-sast-${{ matrix.language }}.json", 'w') as f:
                  json.dump(unified_report, f, indent=2)
              
              return unified_report

          def map_severity(severity):
              severity_map = {
                  "ERROR": "high",
                  "WARNING": "medium",
                  "INFO": "low",
                  "CRITICAL": "critical"
              }
              return severity_map.get(severity.upper(), "info")

          def map_bandit_severity(severity):
              severity_map = {
                  "HIGH": "high",
                  "MEDIUM": "medium",
                  "LOW": "low"
              }
              return severity_map.get(severity.upper(), "info")

          if __name__ == "__main__":
              report = process_sast_reports()
              print(f"SAST Analysis Complete - Total Issues: {report['summary']['total_issues']}")
              print(f"Critical: {report['summary']['critical']}, High: {report['summary']['high']}, Medium: {report['summary']['medium']}")
          EOF
          
          python process_sast.py

      - name: Upload SAST Artifacts - ${{ matrix.language }}
        uses: actions/upload-artifact@v4
        with:
          name: sast-reports-${{ matrix.language }}
          path: sast-reports/
          retention-days: 30

  # Phase 3: Software Composition Analysis (SCA)
  sca-analysis:
    name: ðŸ“¦ SCA Analysis
    runs-on: ubuntu-latest
    needs: security-preparation
    if: ${{ github.event.inputs.scan_type == 'sca-only' || github.event.inputs.scan_type == null || github.event.inputs.scan_type == '' || github.event.inputs.scan_type == 'full' }}
    timeout-minutes: 20
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Install SCA Tools
        run: |
          # Install Trivy
          sudo apt-get update
          sudo apt-get install wget apt-transport-https gnupg lsb-release -y
          wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -
          echo "deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main" | sudo tee -a /etc/apt/sources.list.d/trivy.list
          sudo apt-get update
          sudo apt-get install trivy -y
          
          # Install additional SCA tools
          pip install safety cyclonedx-bom

      - name: Generate Software Bill of Materials (SBOM)
        run: |
          mkdir -p sca-reports
          
          # Generate SBOM for different package managers
          if [ -f package.json ]; then
            echo "ðŸ” Generating Node.js SBOM"
            cyclonedx-node --output-format json --output-file sca-reports/sbom-nodejs.json . || true
          fi
          
          if [ -f requirements.txt ] || [ -f pyproject.toml ]; then
            echo "ðŸ” Generating Python SBOM"
            cyclonedx-py --output-format json --output-file sca-reports/sbom-python.json . || true
          fi
          
          if [ -f pom.xml ]; then
            echo "ðŸ” Generating Maven SBOM"
            mvn org.cyclonedx:cyclonedx-maven-plugin:makeAggregateBom || true
          fi
          
          if [ -f go.mod ]; then
            echo "ðŸ” Generating Go SBOM"
            # Go SBOM generation would go here
            echo "Go SBOM generation - to be implemented" > sca-reports/sbom-go.txt
          fi

      - name: Run Vulnerability Scanning
        run: |
          echo "ðŸ” Running vulnerability scans"
          
          # Trivy filesystem scan
          trivy fs --format json --output sca-reports/trivy-fs.json . || true
          
          # Trivy config scan
          trivy config --format json --output sca-reports/trivy-config.json . || true
          
          # Language-specific dependency scans
          if [ -f package.json ]; then
            npm audit --audit-level moderate --json > sca-reports/npm-audit.json || true
            trivy fs --format json --output sca-reports/trivy-npm.json package-lock.json || true
          fi
          
          if [ -f requirements.txt ]; then
            safety check --json --output sca-reports/safety-python.json || true
            trivy fs --format json --output sca-reports/trivy-python.json requirements.txt || true
          fi
          
          if [ -f pom.xml ]; then
            mvn dependency:tree -DoutputFile=sca-reports/maven-dependencies.txt || true
            trivy fs --format json --output sca-reports/trivy-maven.json pom.xml || true
          fi

      - name: Process SCA Results
        run: |
          cat > process_sca.py << 'EOF'
          import json
          import os
          import glob
          from datetime import datetime

          def process_sca_reports():
              unified_report = {
                  "scan_id": "${{ needs.security-preparation.outputs.scan-id }}",
                  "scan_type": "SCA",
                  "timestamp": datetime.utcnow().isoformat() + "Z",
                  "vulnerabilities": [],
                  "licenses": [],
                  "dependencies": [],
                  "summary": {
                      "total_vulnerabilities": 0,
                      "critical": 0,
                      "high": 0,
                      "medium": 0,
                      "low": 0,
                      "total_dependencies": 0,
                      "license_issues": 0
                  },
                  "tools_used": []
              }
              
              # Process Trivy results
              for trivy_file in glob.glob("sca-reports/trivy-*.json"):
                  try:
                      with open(trivy_file, 'r') as f:
                          data = json.load(f)
                      
                      unified_report["tools_used"].append("trivy")
                      
                      for result in data.get("Results", []):
                          for vuln in result.get("Vulnerabilities", []):
                              vulnerability = {
                                  "tool": "trivy",
                                  "cve_id": vuln.get("VulnerabilityID", "unknown"),
                                  "package": vuln.get("PkgName", "unknown"),
                                  "version": vuln.get("InstalledVersion", "unknown"),
                                  "severity": vuln.get("Severity", "UNKNOWN").lower(),
                                  "title": vuln.get("Title", ""),
                                  "description": vuln.get("Description", ""),
                                  "fixed_version": vuln.get("FixedVersion", ""),
                                  "references": vuln.get("References", [])
                              }
                              unified_report["vulnerabilities"].append(vulnerability)
                              unified_report["summary"]["total_vulnerabilities"] += 1
                              unified_report["summary"][vulnerability["severity"]] += 1
                  except Exception as e:
                      print(f"Error processing {trivy_file}: {e}")
              
              # Process NPM Audit results
              if os.path.exists("sca-reports/npm-audit.json"):
                  try:
                      with open("sca-reports/npm-audit.json", 'r') as f:
                          data = json.load(f)
                      
                      unified_report["tools_used"].append("npm-audit")
                      
                      for vuln_id, vuln_data in data.get("vulnerabilities", {}).items():
                          vulnerability = {
                              "tool": "npm-audit",
                              "cve_id": vuln_data.get("name", vuln_id),
                              "package": vuln_data.get("name", "unknown"),
                              "severity": vuln_data.get("severity", "unknown").lower(),
                              "title": vuln_data.get("title", ""),
                              "description": vuln_data.get("overview", ""),
                              "references": vuln_data.get("references", [])
                          }
                          unified_report["vulnerabilities"].append(vulnerability)
                          unified_report["summary"]["total_vulnerabilities"] += 1
                          unified_report["summary"][vulnerability["severity"]] += 1
                  except Exception as e:
                      print(f"Error processing npm-audit.json: {e}")
              
              # Save unified report
              with open("sca-reports/unified-sca.json", 'w') as f:
                  json.dump(unified_report, f, indent=2)
              
              return unified_report

          if __name__ == "__main__":
              report = process_sca_reports()
              print(f"SCA Analysis Complete - Total Vulnerabilities: {report['summary']['total_vulnerabilities']}")
              print(f"Critical: {report['summary']['critical']}, High: {report['summary']['high']}, Medium: {report['summary']['medium']}")
          EOF
          
          python process_sca.py

      - name: Upload SCA Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: sca-reports
          path: sca-reports/
          retention-days: 30

  # Phase 4: Secrets Detection
  secrets-scanning:
    name: ðŸ” Secrets Detection
    runs-on: ubuntu-latest
    needs: security-preparation
    if: ${{ github.event.inputs.scan_type == 'secrets-only' || github.event.inputs.scan_type == null || github.event.inputs.scan_type == '' || github.event.inputs.scan_type == 'full' }}
    timeout-minutes: 15
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Need full history for secrets scanning

      - name: Install Secrets Detection Tools
        run: |
          # Install TruffleHog
          curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin
          
          # Install GitLeaks
          wget -O gitleaks.tar.gz https://github.com/zricethezav/gitleaks/releases/latest/download/gitleaks_linux_x64.tar.gz
          tar -xzf gitleaks.tar.gz
          sudo mv gitleaks /usr/local/bin/
          
          # Install detect-secrets
          pip install detect-secrets

      - name: Run Secrets Scanning
        run: |
          mkdir -p secrets-reports
          
          echo "ðŸ” Running secrets detection"
          
          # TruffleHog scan
          trufflehog filesystem . --json > secrets-reports/trufflehog.json || true
          
          # GitLeaks scan
          gitleaks detect --source . --report-format json --report-path secrets-reports/gitleaks.json || true
          
          # detect-secrets scan
          detect-secrets scan --baseline secrets-reports/detect-secrets.json . || true
          
          # Custom secrets patterns
          cat > custom_secrets.py << 'EOF'
          import re
          import os
          import json
          from datetime import datetime

          def scan_for_custom_patterns():
              patterns = {
                  'api_key': r'(?i)api[_-]?key[\'"\s]*[:=][\'"\s]*[a-zA-Z0-9_\-]{20,}',
                  'aws_access_key': r'AKIA[0-9A-Z]{16}',
                  'slack_token': r'xox[baprs]-[0-9a-zA-Z\-]{10,}',
                  'github_token': r'ghp_[a-zA-Z0-9]{36}',
                  'jwt_token': r'eyJ[a-zA-Z0-9_\-]*\.[a-zA-Z0-9_\-]*\.[a-zA-Z0-9_\-]*',
                  'private_key': r'-----BEGIN [A-Z]+ PRIVATE KEY-----',
                  'connection_string': r'(?i)(password|pwd)[\'"\s]*[:=][\'"\s]*[^\s\'"]+',
              }
              
              findings = []
              
              for root, dirs, files in os.walk('.'):
                  # Skip common ignore directories
                  dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['node_modules', '__pycache__', 'target', 'build', 'dist']]
                  
                  for file in files:
                      if file.endswith(('.txt', '.json', '.yml', '.yaml', '.env', '.config', '.properties', '.js', '.py', '.java', '.go', '.cs')):
                          file_path = os.path.join(root, file)
                          try:
                              with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                                  content = f.read()
                              
                              for pattern_name, pattern in patterns.items():
                                  matches = re.findall(pattern, content, re.MULTILINE)
                                  for match in matches:
                                      findings.append({
                                          'file': file_path,
                                          'pattern': pattern_name,
                                          'match': match[:50] + '...' if len(match) > 50 else match,
                                          'line': content[:content.find(match)].count('\n') + 1
                                      })
                          except Exception as e:
                              pass
              
              return findings

          if __name__ == "__main__":
              custom_findings = scan_for_custom_patterns()
              with open('secrets-reports/custom-secrets.json', 'w') as f:
                  json.dump({
                      'timestamp': datetime.utcnow().isoformat() + 'Z',
                      'findings': custom_findings,
                      'total_findings': len(custom_findings)
                  }, f, indent=2)
              print(f"Custom secrets scan complete - {len(custom_findings)} potential secrets found")
          EOF
          
          python custom_secrets.py

      - name: Process Secrets Results
        run: |
          cat > process_secrets.py << 'EOF'
          import json
          import os
          import glob
          from datetime import datetime

          def process_secrets_reports():
              unified_report = {
                  "scan_id": "${{ needs.security-preparation.outputs.scan-id }}",
                  "scan_type": "secrets",
                  "timestamp": datetime.utcnow().isoformat() + "Z",
                  "secrets_found": [],
                  "summary": {
                      "total_secrets": 0,
                      "high_confidence": 0,
                      "medium_confidence": 0,
                      "low_confidence": 0,
                      "files_scanned": 0
                  },
                  "tools_used": []
              }
              
              # Process TruffleHog results
              if os.path.exists("secrets-reports/trufflehog.json"):
                  try:
                      with open("secrets-reports/trufflehog.json", 'r') as f:
                          for line in f:
                              if line.strip():
                                  data = json.loads(line)
                      
                      unified_report["tools_used"].append("trufflehog")
                      
                      secret = {
                          "tool": "trufflehog",
                          "detector_type": data.get("DetectorType", "unknown"),
                          "file": data.get("SourceMetadata", {}).get("Data", {}).get("Filesystem", {}).get("file", "unknown"),
                          "line": data.get("SourceMetadata", {}).get("Data", {}).get("Filesystem", {}).get("line", 0),
                          "confidence": "high" if data.get("Verified") else "medium",
                          "redacted": data.get("Redacted", ""),
                      }
                      unified_report["secrets_found"].append(secret)
                      unified_report["summary"]["total_secrets"] += 1
                      unified_report["summary"][secret["confidence"] + "_confidence"] += 1
                  except Exception as e:
                      print(f"Error processing trufflehog.json: {e}")
              
              # Process GitLeaks results
              if os.path.exists("secrets-reports/gitleaks.json"):
                  try:
                      with open("secrets-reports/gitleaks.json", 'r') as f:
                          data = json.load(f)
                      
                      unified_report["tools_used"].append("gitleaks")
                      
                      for finding in data:
                          secret = {
                              "tool": "gitleaks",
                              "detector_type": finding.get("RuleID", "unknown"),
                              "file": finding.get("File", "unknown"),
                              "line": finding.get("StartLine", 0),
                              "confidence": "medium",
                              "redacted": finding.get("Secret", "")[:20] + "..." if finding.get("Secret") else "",
                              "commit": finding.get("Commit", "")
                          }
                          unified_report["secrets_found"].append(secret)
                          unified_report["summary"]["total_secrets"] += 1
                          unified_report["summary"]["medium_confidence"] += 1
                  except Exception as e:
                      print(f"Error processing gitleaks.json: {e}")
              
              # Process custom secrets results
              if os.path.exists("secrets-reports/custom-secrets.json"):
                  try:
                      with open("secrets-reports/custom-secrets.json", 'r') as f:
                          data = json.load(f)
                      
                      unified_report["tools_used"].append("custom-patterns")
                      
                      for finding in data.get("findings", []):
                          secret = {
                              "tool": "custom-patterns",
                              "detector_type": finding.get("pattern", "unknown"),
                              "file": finding.get("file", "unknown"),
                              "line": finding.get("line", 0),
                              "confidence": "low",
                              "redacted": finding.get("match", "")
                          }
                          unified_report["secrets_found"].append(secret)
                          unified_report["summary"]["total_secrets"] += 1
                          unified_report["summary"]["low_confidence"] += 1
                  except Exception as e:
                      print(f"Error processing custom-secrets.json: {e}")
              
              # Save unified report
              with open("secrets-reports/unified-secrets.json", 'w') as f:
                  json.dump(unified_report, f, indent=2)
              
              return unified_report

          if __name__ == "__main__":
              report = process_secrets_reports()
              print(f"Secrets Scanning Complete - Total Secrets: {report['summary']['total_secrets']}")
              print(f"High Confidence: {report['summary']['high_confidence']}, Medium: {report['summary']['medium_confidence']}, Low: {report['summary']['low_confidence']}")
          EOF
          
          python process_secrets.py

      - name: Upload Secrets Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: secrets-reports
          path: secrets-reports/
          retention-days: 30

  # Phase 5: Dynamic Application Security Testing (DAST) - Optional
  dast-analysis:
    name: ðŸŒ DAST Analysis
    runs-on: ubuntu-latest
    needs: security-preparation
    if: ${{ github.event.inputs.scan_type == 'dast-only' || github.event.inputs.scan_type == null || github.event.inputs.scan_type == '' || github.event.inputs.scan_type == 'full' }}
    timeout-minutes: 45
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Check for Web Application
        id: check-webapp
        run: |
          HAS_WEBAPP="false"
          
          # Check for common web application indicators
          if [ -f "package.json" ]; then
            if grep -q "react\|vue\|angular\|express\|next" package.json; then
              HAS_WEBAPP="true"
            fi
          fi
          
          if [ -f "requirements.txt" ]; then
            if grep -q "django\|flask\|fastapi" requirements.txt; then
              HAS_WEBAPP="true"
            fi
          fi
          
          if [ -f "pom.xml" ]; then
            if grep -q "spring-boot\|spring-web" pom.xml; then
              HAS_WEBAPP="true"
            fi
          fi
          
          echo "has-webapp=$HAS_WEBAPP" >> $GITHUB_OUTPUT
          echo "ðŸŒ Web Application Detected: $HAS_WEBAPP"

      - name: Setup DAST Environment
        if: steps.check-webapp.outputs.has-webapp == 'true'
        run: |
          # Install OWASP ZAP
          wget -O zap.tar.gz https://github.com/zaproxy/zaproxy/releases/download/v2.14.0/ZAP_2_14_0_unix.sh
          chmod +x ZAP_2_14_0_unix.sh
          ./ZAP_2_14_0_unix.sh -q
          
          # Install additional DAST tools
          pip install requests beautifulsoup4

      - name: Start Application for DAST
        if: steps.check-webapp.outputs.has-webapp == 'true'
        run: |
          # This would start the application based on the detected type
          # For now, we'll simulate this with a placeholder
          echo "ðŸš€ Starting application for DAST testing..."
          
          # Create a simple test server for demonstration
          cat > test_server.py << 'EOF'
          from http.server import HTTPServer, SimpleHTTPRequestHandler
          import threading
          import time

          def start_server():
              server = HTTPServer(('localhost', 8080), SimpleHTTPRequestHandler)
              server.serve_forever()

          if __name__ == "__main__":
              thread = threading.Thread(target=start_server)
              thread.daemon = True
              thread.start()
              print("Test server started on http://localhost:8080")
              time.sleep(300)  # Keep alive for 5 minutes
          EOF
          
          python test_server.py &
          sleep 10  # Wait for server to start
          
          # Verify server is running
          curl -f http://localhost:8080 || echo "Server not accessible for DAST testing"

      - name: Run DAST Scan
        if: steps.check-webapp.outputs.has-webapp == 'true'
        run: |
          mkdir -p dast-reports
          
          echo "ðŸ” Running DAST analysis"
          
          # OWASP ZAP Baseline Scan
          /opt/zaproxy/zap.sh -cmd -quickurl http://localhost:8080 -quickout dast-reports/zap-baseline.json || true
          
          # Custom DAST checks
          cat > custom_dast.py << 'EOF'
          import requests
          import json
          from datetime import datetime

          def run_basic_dast_checks(base_url):
              findings = []
              
              # Test common security headers
              try:
                  response = requests.get(base_url, timeout=10)
                  headers = response.headers
                  
                  # Check for security headers
                  security_headers = {
                      'X-Content-Type-Options': 'nosniff',
                      'X-Frame-Options': ['DENY', 'SAMEORIGIN'],
                      'X-XSS-Protection': '1; mode=block',
                      'Strict-Transport-Security': '',
                      'Content-Security-Policy': ''
                  }
                  
                  for header, expected in security_headers.items():
                      if header not in headers:
                          findings.append({
                              'type': 'Missing Security Header',
                              'severity': 'medium',
                              'description': f'Missing {header} header',
                              'recommendation': f'Add {header} header to responses'
                          })
                      elif expected and isinstance(expected, list) and headers.get(header) not in expected:
                          findings.append({
                              'type': 'Weak Security Header',
                              'severity': 'low',
                              'description': f'{header} header has weak value: {headers.get(header)}',
                              'recommendation': f'Use recommended values: {expected}'
                          })
                  
                  # Check for server information disclosure
                  if 'Server' in headers:
                      findings.append({
                          'type': 'Information Disclosure',
                          'severity': 'low',
                          'description': f'Server header reveals: {headers["Server"]}',
                          'recommendation': 'Remove or minimize server information in headers'
                      })
                  
              except Exception as e:
                  findings.append({
                      'type': 'Connection Error',
                      'severity': 'info',
                      'description': f'Could not connect to {base_url}: {str(e)}',
                      'recommendation': 'Ensure application is running for DAST testing'
                  })
              
              return findings

          if __name__ == "__main__":
              findings = run_basic_dast_checks("http://localhost:8080")
              report = {
                  'timestamp': datetime.utcnow().isoformat() + 'Z',
                  'target_url': 'http://localhost:8080',
                  'findings': findings,
                  'total_findings': len(findings)
              }
              
              with open('dast-reports/custom-dast.json', 'w') as f:
                  json.dump(report, f, indent=2)
              
              print(f"Custom DAST scan complete - {len(findings)} findings")
          EOF
          
          python custom_dast.py

      - name: Process DAST Results
        if: steps.check-webapp.outputs.has-webapp == 'true'
        run: |
          # Process and unify DAST results
          echo "ðŸ“Š Processing DAST results"
          
          if [ -f dast-reports/custom-dast.json ]; then
            echo "Custom DAST findings:"
            cat dast-reports/custom-dast.json
          fi
          
          # Create unified DAST report
          echo '{"scan_type": "DAST", "status": "completed", "findings": []}' > dast-reports/unified-dast.json

      - name: Upload DAST Artifacts
        if: steps.check-webapp.outputs.has-webapp == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: dast-reports
          path: dast-reports/
          retention-days: 30

  # Phase 6: Security Report Aggregation
  security-aggregation:
    name: ðŸ“Š Security Report Aggregation
    runs-on: ubuntu-latest
    needs: [security-preparation, sast-analysis, sca-analysis, secrets-scanning, dast-analysis]
    if: always() && needs.security-preparation.result == 'success'
    timeout-minutes: 15
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Download All Security Reports
        uses: actions/download-artifact@v4
        with:
          path: all-security-reports

      - name: Aggregate Security Results
        run: |
          mkdir -p final-security-report
          
          cat > aggregate_security.py << 'EOF'
          import json
          import os
          import glob
          from datetime import datetime

          def aggregate_security_reports():
              aggregated_report = {
                  "scan_id": "${{ needs.security-preparation.outputs.scan-id }}",
                  "repository": "${{ github.repository }}",
                  "branch": "${{ github.ref_name }}",
                  "commit": "${{ github.sha }}",
                  "timestamp": datetime.utcnow().isoformat() + "Z",
                  "scan_type": "${{ env.QMS_SCAN_TYPE }}",
                  "summary": {
                      "total_issues": 0,
                      "critical": 0,
                      "high": 0,
                      "medium": 0,
                      "low": 0,
                      "info": 0,
                      "sast_issues": 0,
                      "sca_vulnerabilities": 0,
                      "secrets_found": 0,
                      "dast_findings": 0
                  },
                  "compliance_status": "unknown",
                  "risk_score": 0,
                  "recommendations": [],
                  "detailed_findings": {
                      "sast": [],
                      "sca": [],
                      "secrets": [],
                      "dast": []
                  },
                  "tools_used": []
              }
              
              # Process SAST reports
              for sast_file in glob.glob("all-security-reports/sast-reports-*/unified-sast-*.json"):
                  try:
                      with open(sast_file, 'r') as f:
                          data = json.load(f)
                      aggregated_report["detailed_findings"]["sast"].extend(data.get("findings", []))
                      aggregated_report["summary"]["sast_issues"] += data.get("summary", {}).get("total_issues", 0)
                      aggregated_report["tools_used"].extend(data.get("tools_used", []))
                  except Exception as e:
                      print(f"Error processing {sast_file}: {e}")
              
              # Process SCA reports
              if os.path.exists("all-security-reports/sca-reports/unified-sca.json"):
                  try:
                      with open("all-security-reports/sca-reports/unified-sca.json", 'r') as f:
                          data = json.load(f)
                      aggregated_report["detailed_findings"]["sca"].extend(data.get("vulnerabilities", []))
                      aggregated_report["summary"]["sca_vulnerabilities"] += data.get("summary", {}).get("total_vulnerabilities", 0)
                      aggregated_report["tools_used"].extend(data.get("tools_used", []))
                  except Exception as e:
                      print(f"Error processing SCA report: {e}")
              
              # Process Secrets reports
              if os.path.exists("all-security-reports/secrets-reports/unified-secrets.json"):
                  try:
                      with open("all-security-reports/secrets-reports/unified-secrets.json", 'r') as f:
                          data = json.load(f)
                      aggregated_report["detailed_findings"]["secrets"].extend(data.get("secrets_found", []))
                      aggregated_report["summary"]["secrets_found"] += data.get("summary", {}).get("total_secrets", 0)
                      aggregated_report["tools_used"].extend(data.get("tools_used", []))
                  except Exception as e:
                      print(f"Error processing secrets report: {e}")
              
              # Process DAST reports
              if os.path.exists("all-security-reports/dast-reports/unified-dast.json"):
                  try:
                      with open("all-security-reports/dast-reports/unified-dast.json", 'r') as f:
                          data = json.load(f)
                      aggregated_report["detailed_findings"]["dast"].extend(data.get("findings", []))
                      aggregated_report["summary"]["dast_findings"] += len(data.get("findings", []))
                  except Exception as e:
                      print(f"Error processing DAST report: {e}")
              
              # Calculate severity totals across all scan types
              for scan_type in ["sast", "sca", "secrets", "dast"]:
                  for finding in aggregated_report["detailed_findings"][scan_type]:
                      severity = finding.get("severity", "info").lower()
                      if severity in aggregated_report["summary"]:
                          aggregated_report["summary"][severity] += 1
                      aggregated_report["summary"]["total_issues"] += 1
              
              # Calculate risk score (0-100)
              risk_score = (
                  aggregated_report["summary"]["critical"] * 10 +
                  aggregated_report["summary"]["high"] * 5 +
                  aggregated_report["summary"]["medium"] * 2 +
                  aggregated_report["summary"]["low"] * 1
              )
              aggregated_report["risk_score"] = min(risk_score, 100)
              
              # Determine compliance status
              if aggregated_report["summary"]["critical"] > 0:
                  aggregated_report["compliance_status"] = "non_compliant"
              elif aggregated_report["summary"]["high"] > 5:
                  aggregated_report["compliance_status"] = "at_risk"
              elif aggregated_report["summary"]["medium"] > 10:
                  aggregated_report["compliance_status"] = "review_required"
              else:
                  aggregated_report["compliance_status"] = "compliant"
              
              # Generate recommendations
              if aggregated_report["summary"]["critical"] > 0:
                  aggregated_report["recommendations"].append("URGENT: Address all critical security issues immediately")
              if aggregated_report["summary"]["secrets_found"] > 0:
                  aggregated_report["recommendations"].append("Remove or rotate exposed secrets and credentials")
              if aggregated_report["summary"]["sca_vulnerabilities"] > 0:
                  aggregated_report["recommendations"].append("Update vulnerable dependencies to patched versions")
              if aggregated_report["summary"]["high"] > 0:
                  aggregated_report["recommendations"].append("Prioritize fixing high-severity security issues")
              
              # Remove duplicates from tools_used
              aggregated_report["tools_used"] = list(set(aggregated_report["tools_used"]))
              
              # Save final report
              with open("final-security-report/qms-security-report.json", 'w') as f:
                  json.dump(aggregated_report, f, indent=2)
              
              return aggregated_report

          if __name__ == "__main__":
              report = aggregate_security_reports()
              print("=" * 60)
              print("ðŸ›¡ï¸  QMS SECURITY SCAN SUMMARY")
              print("=" * 60)
              print(f"ðŸ“Š Total Issues: {report['summary']['total_issues']}")
              print(f"ðŸ”´ Critical: {report['summary']['critical']}")
              print(f"ðŸŸ  High: {report['summary']['high']}")
              print(f"ðŸŸ¡ Medium: {report['summary']['medium']}")
              print(f"ðŸŸ¢ Low: {report['summary']['low']}")
              print(f"ðŸ”’ Secrets Found: {report['summary']['secrets_found']}")
              print(f"ðŸ“¦ SCA Vulnerabilities: {report['summary']['sca_vulnerabilities']}")
              print(f"âš¡ Risk Score: {report['risk_score']}/100")
              print(f"ðŸ›ï¸ Compliance Status: {report['compliance_status']}")
              print("=" * 60)
              
              if report['recommendations']:
                  print("ðŸ’¡ RECOMMENDATIONS:")
                  for i, rec in enumerate(report['recommendations'], 1):
                      print(f"   {i}. {rec}")
                  print("=" * 60)
          EOF
          
          python aggregate_security.py

      - name: Generate Security Report HTML
        run: |
          cat > generate_html_report.py << 'EOF'
          import json
          from datetime import datetime

          def generate_html_report():
              with open("final-security-report/qms-security-report.json", 'r') as f:
                  data = json.load(f)
              
              html = f"""
              <!DOCTYPE html>
              <html>
              <head>
                  <title>QMS Security Report - {data['repository']}</title>
                  <style>
                      body {{ font-family: Arial, sans-serif; margin: 20px; }}
                      .header {{ background: #2c3e50; color: white; padding: 20px; border-radius: 5px; }}
                      .summary {{ display: flex; gap: 20px; margin: 20px 0; }}
                      .metric {{ background: #ecf0f1; padding: 15px; border-radius: 5px; text-align: center; flex: 1; }}
                      .critical {{ background: #e74c3c; color: white; }}
                      .high {{ background: #f39c12; color: white; }}
                      .medium {{ background: #f1c40f; color: black; }}
                      .low {{ background: #27ae60; color: white; }}
                      .compliance {{ padding: 15px; margin: 20px 0; border-radius: 5px; }}
                      .compliant {{ background: #d5e8d4; border-left: 5px solid #27ae60; }}
                      .non_compliant {{ background: #f8cecc; border-left: 5px solid #e74c3c; }}
                      .at_risk {{ background: #fff2cc; border-left: 5px solid #f39c12; }}
                      .review_required {{ background: #e1d5e7; border-left: 5px solid #8e44ad; }}
                      .findings {{ margin: 20px 0; }}
                      .finding {{ background: #f8f9fa; margin: 10px 0; padding: 15px; border-radius: 5px; border-left: 4px solid #6c757d; }}
                      .recommendations {{ background: #d4edda; padding: 15px; border-radius: 5px; margin: 20px 0; }}
                  </style>
              </head>
              <body>
                  <div class="header">
                      <h1>ðŸ›¡ï¸ QMS Security Report</h1>
                      <p><strong>Repository:</strong> {data['repository']}</p>
                      <p><strong>Branch:</strong> {data['branch']}</p>
                      <p><strong>Scan ID:</strong> {data['scan_id']}</p>
                      <p><strong>Timestamp:</strong> {data['timestamp']}</p>
                  </div>
                  
                  <div class="summary">
                      <div class="metric critical">
                          <h3>{data['summary']['critical']}</h3>
                          <p>Critical</p>
                      </div>
                      <div class="metric high">
                          <h3>{data['summary']['high']}</h3>
                          <p>High</p>
                      </div>
                      <div class="metric medium">
                          <h3>{data['summary']['medium']}</h3>
                          <p>Medium</p>
                      </div>
                      <div class="metric low">
                          <h3>{data['summary']['low']}</h3>
                          <p>Low</p>
                      </div>
                      <div class="metric">
                          <h3>{data['risk_score']}/100</h3>
                          <p>Risk Score</p>
                      </div>
                  </div>
                  
                  <div class="compliance {data['compliance_status']}">
                      <h3>ðŸ›ï¸ Compliance Status: {data['compliance_status'].replace('_', ' ').title()}</h3>
                  </div>
                  
                  <div class="recommendations">
                      <h3>ðŸ’¡ Recommendations</h3>
                      <ul>
              """
              
              for rec in data['recommendations']:
                  html += f"<li>{rec}</li>"
              
              html += """
                      </ul>
                  </div>
                  
                  <div class="findings">
                      <h3>ðŸ” Key Findings Summary</h3>
              """
              
              # Add key findings summary
              html += f"""
                      <div class="finding">
                          <h4>ðŸ“Š SAST Analysis</h4>
                          <p>{data['summary']['sast_issues']} static analysis issues found across codebase</p>
                      </div>
                      <div class="finding">
                          <h4>ðŸ“¦ SCA Analysis</h4>
                          <p>{data['summary']['sca_vulnerabilities']} vulnerabilities found in dependencies</p>
                      </div>
                      <div class="finding">
                          <h4>ðŸ” Secrets Detection</h4>
                          <p>{data['summary']['secrets_found']} potential secrets or credentials detected</p>
                      </div>
                      <div class="finding">
                          <h4>ðŸŒ DAST Analysis</h4>
                          <p>{data['summary']['dast_findings']} dynamic security findings</p>
                      </div>
                  </div>
                  
                  <div style="margin-top: 40px; text-align: center; color: #7f8c8d;">
                      <p>Generated by QMS Security Pipeline v{data.get('version', '2.3.0')}</p>
                      <p>Tools Used: {', '.join(data['tools_used'])}</p>
                  </div>
              </body>
              </html>
              """
              
              with open("final-security-report/security-report.html", 'w') as f:
                  f.write(html)

          if __name__ == "__main__":
              generate_html_report()
              print("ðŸ“„ HTML report generated: security-report.html")
          EOF
          
          python generate_html_report.py

      - name: Upload Final Security Report
        uses: actions/upload-artifact@v4
        with:
          name: qms-security-report
          path: final-security-report/
          retention-days: 90

      - name: Update Security Dashboard
        if: always()
        run: |
          # Update QMS Dashboard with security results
          curl -X POST "${{ env.QMS_DASHBOARD_ENDPOINT }}/api/security/results" \
            -H "Authorization: Bearer ${{ secrets.QMS_API_TOKEN }}" \
            -H "Content-Type: application/json" \
            -d @final-security-report/qms-security-report.json || true

  # Phase 7: Compliance Check
  compliance-check:
    name: ðŸ›ï¸ Compliance Validation
    runs-on: ubuntu-latest
    needs: [security-preparation, security-aggregation]
    if: ${{ always() && needs.security-preparation.outputs.compliance-required == 'true' && (github.event.inputs.scan_type == 'compliance-only' || github.event.inputs.scan_type == null || github.event.inputs.scan_type == '' || github.event.inputs.scan_type == 'full') }}
    timeout-minutes: 10
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Download Security Report
        uses: actions/download-artifact@v4
        with:
          name: qms-security-report
          path: security-report

      - name: Run Compliance Validation
        run: |
          cat > compliance_check.py << 'EOF'
          import json
          from datetime import datetime

          def validate_compliance():
              # Load security report
              with open("security-report/qms-security-report.json", 'r') as f:
                  security_data = json.load(f)
              
              compliance_report = {
                  "scan_id": security_data["scan_id"],
                  "timestamp": datetime.utcnow().isoformat() + "Z",
                  "compliance_framework": "OWASP-ASVS-4.0",
                  "validation_results": [],
                  "overall_status": "unknown",
                  "recommendations": [],
                  "risk_assessment": {
                      "score": security_data["risk_score"],
                      "level": "unknown",
                      "factors": []
                  }
              }
              
              # OWASP ASVS Level 1 Requirements
              validation_rules = [
                  {
                      "rule_id": "V1.1.1",
                      "description": "No critical vulnerabilities in authentication",
                      "check": lambda: security_data["summary"]["critical"] == 0,
                      "weight": "critical"
                  },
                  {
                      "rule_id": "V2.1.1",
                      "description": "No hardcoded secrets in codebase",
                      "check": lambda: security_data["summary"]["secrets_found"] == 0,
                      "weight": "high"
                  },
                  {
                      "rule_id": "V5.1.1",
                      "description": "Limited high-severity input validation issues",
                      "check": lambda: security_data["summary"]["high"] <= 3,
                      "weight": "high"
                  },
                  {
                      "rule_id": "V9.1.1",
                      "description": "No critical dependency vulnerabilities",
                      "check": lambda: sum(1 for vuln in security_data["detailed_findings"]["sca"] if vuln.get("severity") == "critical") == 0,